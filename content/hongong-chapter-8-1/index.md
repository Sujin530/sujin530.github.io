---
emoji: 📝
title: 인공신경망: 합성곱 신경망
date: '2022-02-06 14:40:00'
author: Sujin
tags: 혼공머신 머신러닝 딥러닝 합성곱신경망
categories: 노트
---

### 참고도서
* 혼자 공부하는 머신러닝 + 딥러닝 
* 이토록 쉬운 머신러닝 & 딥러닝 입문 with 사이킷런 + 파이토치

## 인공신경망: 합성곱 신경망
### 학습 목표
* 이미지 분류 문제에 뛰어난 성능을 발휘하는 합성곱 신경망의 개념과 구성 요소에 대해 배우기
* 케라스 API로 합성곱 신경망을 만들어 패션 MNIST 데이터에서 성능을 평가해보기
* 합성곱 층의 필터와 활성화 출력을 시각화하여 합성곱 신경망이 학습한 내용을 고찰해보기


### 합성곱 신경망(CNN, Convolutional Neural Network)
* 합성곱층으로 구성된 딥러닝 모델
* 일반적으로 1개 이상의 합성곱 층을 쓴 인공 신경망을 의미
* 입력 데이터에서 유용한 특성만 드러나게 하는 것

### 합성곱 신경망의 특징
* 합성곱은 일부에 가중치를 곱합니다. 
* 합성곱 층의 뉴런에 있는 가중치 개수는 정하기 나름입니다. 가중치 개수 = 하이퍼파라미터
* 뉴런 = 필터 = 커널
* 합성곱의 장점은 1차원이 아니라 2차원 입력에도 적용할 수 있다는 것!

### 특성 맵(Feature Map)
* 합성곱 계산을 통해 얻은 출력

### 필터
* 뉴런, 커널과 같은 개념
* 밀집층에서 여러 개의 뉴런을 사용하듯이 합성곱 층에서도 여러 개의 필터를 사용함
* 밀집층에 있는 뉴런의 가중치가 모두 다르듯이 합성곱 층에 있는 필터의 가중치(커널)도 모두 다름

### 합성곱 신경망이 이미지 처리 분야에서 뛰어난 성능을 발휘하는 이유
  * 입력보다 훨씬 작은 크기의 커널을 사용하고 입력 위를 이동하면서 2차원 특성 맵을 만듬
  * 2차원 구조를 그대로 사용하기 때문에 

### 케라스 합성곱 층
* 케라스의 층은 모두 keras.layers 패키지 아래 클래스로 구현되어 있음. 합성곱 층도 마찬가지!
* 입력 위를 이동하는 합성곱은 Conv2D 클래스로 제공됨

### Conv2D 클래스
* 첫 번째 매개변수 = 필터
* kernel_size 매개변수는 필터에 사용할 커널의 크기를 지정
* 필터의 개수와 커널의 크기는 반드시 지정해야 하는 매개변수
* 커널의 크기는 하이퍼 파라미터. 여러 가지 값을 시도해봐야 하지만, 보통은 (3, 3)이나 (5, 5) 크기가 권장됨
* 활성화 함수 지정


<img width="460" alt="8-1 (1)" src="https://user-images.githubusercontent.com/67195038/152688254-8d6f7ab7-1cd3-47a0-9b1c-82f3d09d85c0.png">
## 패딩과 스트라이드
### 패딩(Padding)
* 입력 배열의 주위를 가상의 원소로 채우는 것
* 실제 입력값이 아니기 때문에 패딩은 0으로 채움
* (4, 4) 크기의 입력에 0을 1개 패딩하면 다음과 같은 (6, 6) 크기의 입력이 됨

### 세임 패딩(Same Padding)
* 입력과 특성 맵의 크기를 동일하게 만들기 위해 입력 주위에 0으로 패딩하는 것
* 합성곱 신경망에서는 세임 패딩이 많이 사용됨
* 즉, 입력과 특성 맵의 크기를 동일하게 만드는 경우가 많음

### 밸리드 패딩(Valid Padding)
* 패딩 없이 순수한 입력 배열에서만 합성곱을 하여 특성 맵을 만드는 것
* 밸리드 패딩은 특성 맵의 크기가 줄어들 수 밖에 없음

### 합성곱에서 패딩을 즐겨 사용하는 이유
* 적절한 패딩은 이미지의 주변에 있는 정보를 잃어버리지 않도록 도와줌
* 일반적인 합성곱 신경망에서는 세임 패딩이 많이 사용됨
* 케라스 Conv2D 클래스에서는 padding 매개변수로 패딩을 지정할 수 있음

### 스트라이드(Stride)
* 합성곱에서 이동할 때의 크기를 지정하는 것
* 기본적인 스트라이드는 1. 한 칸씩 이동함.
* 대부분 기본값을 그대로 사용하기 때문에 strides 매개변수를 잘 사용하지 않음

## 풀링(Pooling)
* 합성곱 층에서 만든 특성 맵의 크기로 가로세로 크기를 줄이는 역할을 수행
* 특성 맵의 개수는 줄이지 않음
* 풀링에는 가중치가 없고, 최댓값이나 평균값을 계산하는 역할을 수행함

### 최대 풀링(Max Pooling)
* 가장 큰 값을 고르는 것
* 최대 풀링을 제공하는 클래스는 MaxPooling2D
* 첫번째 매개변수로 풀링의 크기 지정. 대부분 풀링 크기 2.
* strides와 padding 매개변수 제공. 
* strides 기본값은 자동으로 풀링의 크기라서 따로 지정할 필요 없음
* padding의 기본 값은 valid로 패딩 하지 않음

### 평균 풀링(Average Pooling)
* 평균 풀링을 제공하는 클래스는 AveragePooling2D
* 평균 풀링보다 최대 풀링을 많이 사용. 이유는? 평균 풀링이 특성 맵에 있는 중요한 정보를 (평균하여) 희석시킬 수 있기 때문

### 풀링을 사용하는 이유
* 합성곱에서 스트라이드를 크게 하여 특성 맵을 줄이는 것보다 풀링 층에서 크기를 줄이는 것이 경험적으로 더 나은 성능을 내기 때문

## 합성곱 신경망의 전체 구조
### 컬러 이미지를 사용한 합성곱
* 패션 MNIST 데이터는 실제로 흑백 이미지이기 때문에 2차원 배열로 표현
* 컬러이미지는 RGB 채널로 구성되어있기 때문에 컴퓨터는 이를 3차원 배열로 표시함
* 커널 배열의 깊이는 항상 입력의 깊이와 같음

* 입력이나 필터의 차원이 몇 개인지 상관없이 항상 출력은 하나의 값이라는 점. 즉 특성 맵에 있는 한 원소가 채워짐!
* 케라스의 합성곱 층은 항상 이렇게 3차원 입력을 기대함. 패션 MNIST 데이터처럼 흑백 이미지일 경우에는 깊이 차원이 1인 3차원 배열로 변환하여 전달함

* 합성곱 신경망은 너비와 높이는 점점 줄어들고 깊이는 점점 깊어지는 것이 특징임
* 마지막에 출력층 전에 특성 맵을 모두 펼쳐서 밀집층의 입력으로 사용함


<img width="634" alt="8-1 (2)" src="https://user-images.githubusercontent.com/67195038/152688263-4de396db-d356-44b8-b228-1fdc3a34dce4.png">

## 용어 정리
* 필터
* 커널
* 특성 맵